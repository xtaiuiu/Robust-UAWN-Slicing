import time

import numpy as np
import scipy.linalg as lg

from utils.projction import proj_nonnegative_half_space_lb


def gp_x(prob, lam, rho, p, x0, eps=1e-18):
    """
    optimize sub_x by gradient projection method.
    Stepsize rule is generated by
    :param prob: must be a BoundProblem object
    :param stepsize_rule: a callable
    :return: f_opt, x_opt
    """
    alpha, gamma = 1.0, 0.5
    f0 = 1e10
    fx = prob.Lagrangian_value(x0, p, lam, rho)
    x = x0
    n, n_max = 0, 10000
    while abs((f0 - fx)/fx) > eps and n < n_max:
        gx = prob.g_x(x, p, lam, rho)
        print(f"n = {n}, f0 = {f0}, fx = {fx}, norm_gx = {lg.norm(gx)}, alpha = {alpha}")
        # eventually constant stepsize rule
        # while fx > f0 + np.dot(g0, x - x0) + lg.norm(x - x0)**2/(2*alpha):
        #     alpha *= gamma
        alpha = armijo_rule(prob, lam, rho, p, x, fx, gx)
        x0 = x
        f0, g0 = prob.Lagrangian_value(x0, p, lam, rho), prob.g_x(x0, p, lam, rho)
        x = proj_nonnegative_half_space_lb(prob.c, x0 - alpha * g0, prob.B_tot, prob.x_u)
        fx = prob.Lagrangian_value(x, p, lam, rho)
        n += 1
    return fx, x

def armijo_rule(prob, lam, rho, p, x, f, g, beta=0.9, sigma=0.1):
    s = 1.0
    m, m_max = 0, 100
    for m in range(m_max):
        x_k_alpha = proj_nonnegative_half_space_lb(prob.c, x - beta ** m * s * g, prob.B_tot, prob.x_u)
        print(f"sigma: {sigma * np.dot(g, x - x_k_alpha)}")
        if f - prob.Lagrangian_value(x_k_alpha, p, lam, rho) >= sigma * np.dot(g, x - x_k_alpha):
            break
    return beta**m*s


# Optimize the problem by using GPM with eventually constant step size rule
# \min\limits_{x} 0.5 * (a^T x - p)^2 + b^T x
# s.t. x >= c,
#      d^T x = e
def gp_momentum_eventually_constant_step(prob, x0, eps=1e-5):
    alpha, gamma = 0.1, 0.5
    f0, g0 = prob.f(x0), prob.g(x0)
    x = proj_nonnegative_half_space_lb(prob.c, x0 - alpha * g0, prob.B_tot, prob.x_u)
    fx = prob.f(x)
    k, k_max = 1, 100000
    while abs(f0 - fx) > eps and k < k_max:
        print(f"k = {k}, f0 = {f0}, fx = {fx}, norm_gx = {lg.norm(prob.g(x))}, alpha = {alpha}")
        beta = (k-1)/(k+2)
        y = x + beta * (x - x0)
        # eventually constant stepsize rule
        fy, gy = prob.f(y), prob.g(y)
        x_tmp = proj_nonnegative_half_space_lb(prob.c, y - alpha * gy, prob.B_tot, prob.x_u)
        f_tmp = prob.f(x_tmp)
        while f_tmp > fy + np.dot(gy, x_tmp - y) + lg.norm(x_tmp - y)**2/(2*alpha):
            alpha *= gamma
            x_tmp = proj_nonnegative_half_space_lb(prob.c, y - alpha * gy, prob.B_tot, prob.x_u)
            f_tmp = prob.f(x_tmp)
        x0 = x
        x = x_tmp
        f0, fx = prob.f(x0), f_tmp
        k += 1
    print(f"gp finished in {k} iterations")
    return fx, x


# Optimize the problem by using GPM with constant stepsize rule.
# The stepsize \alpha = 1/L.
def gp_momentum_constant_step(prob, lam, rho, p, x0, alpha=500, eps=1e-5):
    f0, g0 = prob.Lagrangian_value(x0, p, lam, rho), prob.g_x(x0, p, lam, rho)
    x = proj_nonnegative_half_space_lb(prob.c, x0 - alpha * g0, prob.B_tot, prob.x_u)
    fx = prob.Lagrangian_value(x, p, lam, rho)
    k, k_max = 1, 50000
    while abs(f0 - fx) > eps and k < k_max:
        print(f"k = {k}, f0 = {f0}, fx = {fx}, alpha = {alpha}")
        beta = (k-1)/(k+2)
        alpha = max(rho/(np.linalg.norm(p)**2), 1/(k+1))
        y = x + beta * (x - x0)  # extrapolation step
        # constant stepsize rule
        fy, gy = prob.Lagrangian_value(y, p, lam, rho), prob.g_x(y, p, lam, rho)
        x_tmp = proj_nonnegative_half_space_lb(prob.c, y - alpha * gy, prob.B_tot, prob.x_u)  # gradient projection step
        x0 = x
        x = x_tmp
        f0 = fx
        fx = prob.Lagrangian_value(x, p, lam, rho)
        k += 1
    print(f"gp constant finished in {k} iterations")
    return fx, x


# Optimize Sub_x by using GPM with eventually constant step size rule
def gpm_x(prob, lam, rho, p, x0, eps=1e-3):
    """
    :param prob: an OptimizationProblem object.
    :param lam: parameter lam in AL problem
    :param rho: parameter rho in AL problem
    :param p: variable p, must be positive
    :param eps: tolerance, i.e., epsilon_1 in my paper.
    :return:
    """
    if np.any(p < 0):
        raise ValueError(f"gpm_x: infeasible subproblem !!!!!!!!!!!! error p = {p} !!!!!!!!!!!!!!!!!!")
    if prob.c@prob.x_u > prob.B_tot:
        raise ValueError(f"gpm_x: !! infeasible subproblem: B_tot = {prob.B_tot}, x_lb_sum = {prob.x_u@prob.c} !!")
    alpha, gamma = 0.1, 0.5
    f0 = prob.Lagrangian_value(x0, p, lam, rho)
    x, fx = x0, f0
    #f0, g0 = prob.objective_function(x0, p, lam, rho), prob.g_x(x0, p, lam, rho)
    k, k_max = 1, 10000
    while k < k_max:
        if k > 1 and abs(f0 - fx) < eps and abs((f0 - fx)/fx) < eps:
            break
        #print(f"GPM: k = {k}, f0 = {f0}, fx = {fx}, alpha = {alpha}")
        beta = (k)/(k+1)
        y = x + beta * (x - x0)
        # eventually constant stepsize rule

        fy, gy = prob.Lagrangian_value(y, p, lam, rho), prob.g_x(y, p, lam, rho)  # f(y_k), \nabla f(y_k)
        x_tmp = proj_nonnegative_half_space_lb(prob.c, y - alpha * prob.g_x(x, p, lam, rho), prob.B_tot, prob.x_u)  # x_{k+1}
        f_tmp = prob.Lagrangian_value(x_tmp, p, lam, rho)  # f(x_{k+1})
        while f_tmp > fy + np.dot(gy, x_tmp - y) + lg.norm(x_tmp - y)**2/(2*alpha):
            alpha *= gamma
            x_tmp = proj_nonnegative_half_space_lb(prob.c, y - alpha * gy, prob.B_tot, prob.x_u)
            f_tmp = prob.Lagrangian_value(x_tmp, p, lam, rho)  # f(x_{k+1})
        x0 = x
        x = x_tmp
        f0, fx = fx, f_tmp
        k += 1
    #print(f"gp finished in {k} iterations")
    return fx, x


# Optimize Sub_p by using GPM with eventually constant step size rule
def gpm_p(prob, lam, rho, x, eps=1e-5):
    """
    :param prob: an OptimizationProblem object.
    :param lam: parameter lam in AL problem
    :param rho: parameter rho in AL problem
    :param x: variable x, must be positive
    :param eps: tolerance, i.e., epsilon_1 in my paper.
    :return:
    """
    alphp_0 = 1
    alpha, gamma = alphp_0/2, 0.5
    p0 = prob.p_u
    f0, g0 = prob.Lagrangian_value(x, p0, lam, rho), prob.g_p(x, p0, lam, rho)
    p = np.maximum(prob.p_u, p0 - alpha * g0)
    f = prob.Lagrangian_value(x, p, lam, rho)
    k, k_max = 1, 2000
    alpha_not_stopped = True
    while abs(f0 - f) > eps and k < k_max:
        beta = (k-1)/(k+2)
        y = p + beta * (p - p0)  # y_k
        # eventually constant stepsize rule
        fy = 1e16 if np.any(y < 0) else prob.Lagrangian_value(x, y, lam, rho)
        gy = prob.g_p(x, y, lam, rho)  # f(y_k), \nabla f(y_k)
        p_next = np.maximum(y - alpha*gy, prob.p_u)  # p_{k+1}
        f_next = prob.Lagrangian_value(x, p_next, lam, rho)  # f(p_{k+1})
        alpha_0 = alpha
        while alpha_not_stopped and f_next > fy + np.dot(gy, p_next - y) + lg.norm(p_next - y)**2/(2*alpha):
            alpha *= gamma
            p_next = np.maximum(y - alpha*gy, prob.p_u)
            f_next = prob.Lagrangian_value(x, p_next, lam, rho)  # f(p_{k+1})
        #print(f"k = {k}, f0 = {f0}, fp = {f}, alpha = {alpha}, beta = {beta}")
        if alpha_0 == alpha:
            alpha_not_stopped = False
        p0 = p
        p = p_next
        f0, f = f, f_next
        #alpha = alphp_0
        k += 1
    #print(f"gp finished in {k} iterations")
    if np.any(p < 0):
        print(f"gpm_p: !!!!!!!!!!!! error p = {p} !!!!!!!!!!!!!!!!!!")
    return f, p


if __name__ == '__main__':
    from algorithms.MP_Relax_algorithms.main_algorithm.DAL.DAL_algorithm import create_optimization_instance
    #prob = create_fixed_optmization_problem(n=10)
    np.random.seed(0)
    prob = create_optimization_instance(n=1000)
    x = prob.x_u
    lam, rho = 0.1, 100
    t1 = time.perf_counter()
    f, p = gpm_p(prob, lam, rho, x)
    print(f"test_gpm_p_random: f_val = {f}, time = {time.perf_counter() - t1}")
    t2 = time.perf_counter()
    # f_val, var = optimize_p_cvx(prob, lam, rho, x)
    # print(f"test_cvx_p_random: f_val = {f_val}, time={time.perf_counter() - t2}")